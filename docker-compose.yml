services:
  # ollama:
  #   image: ollama/ollama
  #   container_name: ollama
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama:/root/.ollama
  #   networks:
  #     - default
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             capabilities: [gpu]
  #             count: all
  #   restart: on-failure

  multilayer_perceptron:
    image: multilayer_perceptron
    container_name: multilayer_perceptron
    build:
      context: .
      dockerfile: Dockerfile.mlp
    stdin_open: true
    tty: true
    networks:
      - default
    restart: on-failure

# volumes:
#   ollama:

networks:
  default:
